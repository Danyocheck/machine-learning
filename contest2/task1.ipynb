{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1. Бинарная кросс-энтропия\n",
    "\n",
    "В сегодняшней домашней работе вам предлагается самостоятельно написать и обучить модель логистической регрессии для решения задачи бинарной классификации.\n",
    "Реализация логистической регрессии с имеющимся заранее вектором весов $\\overrightarrow{ω}$ - задача достаточно простая. Вся загадка кроется в вопросе о том, как подобрать оптимальный вектор весов $\\overrightarrow{ω}$. Прежде чем перейти к вопросу о подборе этого вектора необходимо определиться с тем, в каком смысле мы понимаем его оптимальность. То есть что является критерием его оптимальности? Как сказать, что один вектор $\\overrightarrow{ω_1}$ более оптимален, чем другой $\\overrightarrow{ω}$?\n",
    "\n",
    "Для определения того, насколько хорош некоторый вектор параметров для решения нашей конкретной задачи, вводится понятие функции потерь (Loss-функции). Значение этой функции обратно пропорционально качеству решения нашей задачи. То есть оптимальным вектором параметров мы теперь можем назвать тот, при котором значение Loss-функции будет наименьшим из всех возможных. Таким образом мы сводим решение задачи машинного обучения к решению задачи оптимизации.\n",
    "\n",
    "Для задачи бинарной классификации очень популярна функция потерь, называемая бинарной кросс-энтропией. Эта функция пришла в машинное обучение из теории информации. Она имеет следующий вид:\n",
    " \n",
    "$$H(y,p)=−(y⋅ln(p)+(1−y)⋅ln(1−p))$$\n",
    "\n",
    "Где y - верный ответ (0 или 1), а $p$ - наше предположение, выраженное в оцененной степени принадлежности очередного объекта x классу 1.\n",
    "Для обучения модели мы решим воспользоваться алгоритмом градиентного спуска\n",
    "В связи с этим, нам необходимо предварительно посчитать все производные функции H по параметрам модели $\\overrightarrow{ω_0} ... \\overrightarrow{ω_n}$.\n",
    "\n",
    "Выведите формулы этих производных и посчитайте производную функции H в точке\n",
    "\n",
    "$$ω_0 = 0 \\\\\n",
    "ω_1 = 2 \\\\\n",
    "ω_2 = -1$$\n",
    "\n",
    "По каждому из этих параметров, если рассматриваемый объект $x$, имеющий координаты векторного представления $x_1 = 1 и x_2 = 2$ принадлежит классу 1.\n",
    "\n",
    "В ответ в контесте внесите значение произведения \n",
    "$\\frac{\\partial H}{\\partial ω_0}$ $\\frac{\\partial H}{\\partial ω_1}$ $\\frac{\\partial H}{\\partial ω_2}$\n",
    "​\t\n",
    " \n",
    "# Формат ввода\n",
    "\n",
    "В качестве ответа введите получившееся значение в виде десятичной дроби, в качестве знака десятичной запятой используйте точку. Ограничьтесь двумя значащими цифрами после запятой.\n",
    "\n",
    "## Примечания\n",
    "\n",
    "### Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия - алгоритм мягкой бинарной классификации, суть которого состоит в построении разделяющей гиперплоскости и оценке вероятности принадлежности объекта одному из двух классов на основе значения отступа объекта от построенной гиперплоскости.\n",
    "\n",
    "Построение разделяющей гиперплоскости производится при помощи подбора оптимальных коэффициентов при условии, что решающая функция имеет вид:\n",
    "$$F(x) = [b_\\alpha (\\overrightarrow{x}) ≥ 0] = [ω_0 + ω_1 x_1+ ... +ω_n x_n ≥ 0]$$\n",
    "\n",
    "По знаку выражения в квадратных скобках принимается решение отнести объект к классу 0 или 1.\n",
    "\n",
    "Логистическая регрессия считает степень принадлежности объекта классу 0 и классу 1. Модуль функции \n",
    "$b_ω (\\overrightarrow{x})$ задает расстояние от точки, заданной вектором $\\overrightarrow{x}$ до разделяющей гиперплоскости, заданной совокупностью параметров $\\overrightarrow{ω_0} ... \\overrightarrow{ω_n}$. Монотонное отображение значения $b_ω (\\overrightarrow{x})$ на отрезок [0,1] позволит нам интерпретировать выход этой модели как степень принадлежности (аналог вероятности) объекта x классу 1.\n",
    "\n",
    "Таким отображением традиционно стала сигмоидальная функция:\n",
    "$σ(x) = \\frac{1}{1 + e^{-x}} $\n",
    "​\t\n",
    "Таким образом, окончательная формула, задающая логистическую регрессию, следующая:\n",
    "$$LR(\\overrightarrow{x}) = \\sigma(\\omega_0 + \\omega_1x_1 + ... + \\omega_nx_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия\n",
    "\n",
    "Мы можем также потребовать другое интересное свойство: пусть наш алгоритм возвращает не просто номер класса, а **вероятность принадлежности объекта** тому или иному классу. Тогда мы получаем алгоритм, который носит название логистической регрессии.\n",
    "\n",
    "Идея состоит в том, что наш классификатор должен теперь выдавать не строгие ответы 1/0, а лишь **вероятность** принадлежности объекта классу 1, при этом оставаясь линейным классификатором.\n",
    "\n",
    "Такой способ классификации называется **мягкой классификацией**, то есть классификацией, при которой алгоритм возвращает степень принадлежности объекта одному из классов. Альтернативный более привычный нам способ классификации - **жёсткая классификация**, при которой алгоритм однозначно возвращает нам один из классов.\n",
    "\n",
    "Для того чтобы сделать это, мы воспользуемся очень известной функцией, которую обычно называют [сигмоидой](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=v2VSpP-GGtSq):\n",
    "\n",
    "$$σ(x) = \\frac{1}{1 + e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сигмоида обладает рядом хороших свойств:\n",
    "\n",
    "1) Значения сигмоиды, как нетрудно убедиться, лежат в диапазоне от 0 до 1\n",
    "\n",
    "2) Ось Y график сигмоиды пересекает в точке 0.5\n",
    "\n",
    "3) Функция стремится к 1 на бесконечности и к 0 - на минус бесконечности\n",
    "\n",
    "Пусть алгоритм будет следующим:\n",
    "\n",
    "Вместо обычного вычисления функции $(w,x) + w_0$, по знаку которой мы делаем вывод о принадлежности к классу 0 или классу 1, мы применим к полученному значению функцию сигмоиды.\n",
    "\n",
    "То есть $a(x) = σ((w,x) + w_0)$\n",
    "\n",
    "Обратим внимание вот на что:\n",
    "\n",
    "- Когда наш классификатор принял решение отнести объект к классу 1, $(w,x) + w_0 > 0$, то есть $σ((w,x) + w_0) > 0.5$, а когда объект отнесен к классу 0 - $σ((w,x) + w_0) < 0.5$. То есть 0.5 для нас теперь граница принятия решения\n",
    "- Классификатор теперь выдает нам любое число от 0 до 1\n",
    "- Чем больше отступ, то есть, чем увереннее модель, тем больше будет значение $a(x)$.\n",
    "\n",
    "Все это дает нам повод воспринимать выход $a(x)$ именно как вероятность принадлежности объекта классу 1!\n",
    "Когда эта вероятность больше 0.5 - мы относим объект к классу 1, но эта же ситуация соответствует случаю, когда и модель обычного линейного классификатора, дала нам значение класса 1. Аналогично с 0. При этом выполняются все свойства вероятности.\n",
    "\n",
    "Простое навешивание сигмоиды помогло превратить нашу модель линейного классификатора в модель, возвращающую вероятности. Такая модель называется **логистической регрессией**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели\n",
    "\n",
    "Для всех этих моделей нам необходимо подобрать оптимальный вектор параметров $\\vec{w}$ (и ${w_0}$). В случае SVM мы упомянули, что этот вектор можно найти при помощи некоторой аналитической процедуры, в подробности которой мы вникать не стали. В случае логистической регрессии процедуры, позволяющей определить точный минимум функции ошибки, просто не существует.\n",
    "\n",
    "________________________\n",
    "**Напоминание.**\n",
    "Функция ошибки (Loss-функция, функция потерь) - это функция, зависящая от параметров модели, показывающая насколько сильно эта модель ошибается.\n",
    "\n",
    "Loss-функция зависит не столько от самой модели, сколько от задачи, которую мы решаем.\n",
    "\n",
    "На прошлой лекции мы рассматривали задачу регрессии в контексте метрических методов и обсуждали, что эту задачу можно решать исходя из условия минимизации среднего квадрата ошибки предсказания модели. Это классический пример loss-функции.\n",
    "\n",
    "В задаче бинарной классификации в качестве loss-функции обычно используется бинарная кросс-энтропия (Binary Cross-Entropy, BCE). Эта функция зависит от настоящих меток $y$ и предсказаний алгоритма $a(x) = p$, и выглядит следующим образом:\n",
    "\n",
    "$$H(p, y) = - (y \\cdot ln(p) +(1 - y) \\cdot ln(1-p))$$\n",
    "\n",
    "Эту функцию выбирают исходя из теоретико-информационных соображений.\n",
    "________________________\n",
    "\n",
    "Ясно, что задача обучения модели сводится к подбору таких параметров $\\vec{w}$, которые доставляют минимум ожидаемой функции потерь, в математической записи $\\vec{w} = argmin(Loss_{expected}(\\vec{x}, \\vec{w}))$. Значит, нам нужна какая-то процедура, позволяющая этот минимум найти даже тогда, когда не существует аналитических путей поиска этого минимума. Один из самых популярных алгоритмов такого рода - градиентный спуск.\n",
    "\n",
    "# Градиентный спуск\n",
    "\n",
    "\n",
    "Поиск минимума функции можно сравнить с хождением по горам.\n",
    "\n",
    "Представьте себе, что вы - путешественник, идущий по горе и стремящийся спуститься вниз. В реальности вы будете выбирать свой путь исходя из обзора окружающей местности. Но в случае поиска минимума функции мы можем посчитать только характеристики функции в конкретной точке, поэтому правильной ассоциацией будет случай, когда прежде чем подняться на гору, вы завязали себе глаза. Вы знаете высоту, на которой находитесь, и чувствуете, какой наклон поверхности горы там, где вы находитесь. \n",
    "\n",
    "Что вы будете делать в этом случае? С каждым шагом вы будете идти в ту сторону, куда наклон поверхности наиболее крутой.\n",
    "\n",
    "Эту идею можно распространить на математическую модель поиска минимума. Из математического анализа известно, что если мы имеем функцию $F = F(x_1 ... x_n)$, то вектор [частных производных](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=aGVowH1ufYP_) этой функции (называемый градиентом), $$\\vec{grad(F)} = \\vec{∇F} = (\\frac{\\partial F(x_1)}{\\partial x_1} ... \\frac{\\partial F(x_n)}{\\partial x_n})$$\n",
    "\n",
    "направлен в сторону **наискорейшего роста функции**. Вектор же **антиградиента**, соответственно, направлен в направлении наискорейшего убывания функции. Если мы хотим *шаг за шагом* приближаться к минимуму функции, мы должны каждый раз делать небольшой шажок в направлении антиградиента. Математически эту идею можно записать в виде следующей формулы: $$\\vec{x_{n+1}} = \\vec{x_n} - λ\\vec{∇F(x)}$$ Где $λ$ характеризует размер нашего шага.\n",
    "\n",
    "Применив эту идею для поиска минимума Loss-функции по параметрам нашей модели $\\vec{w}$, мы найдем оптимальный (или почти оптимальный) вектор параметров модели, который и будем использовать при классификации. Запомним эту идею! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производные $\\frac{\\partial H}{\\partial \\omega_i}$\n",
    "\n",
    "В рамках алгоритма градиентного спуска нам необходимо посчитать [производные](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=1m66zhVnUv9w) функции бинарной кросс-энтропии по параметрам модели, чтобы делать шаги градиентного спуска.\n",
    "\n",
    "Покажем, как можно посчитать эту производную.\n",
    "\n",
    "Рассмотрим случай, когда нам необходимо посчитать производные всего для одного примера. То есть мы имеем единственный объект $x$ с истинной меткой $y$ и гипотезой нашего алгоритма $p$. Тогда $$H(p, y) = - (y \\cdot ln(p) +(1 - y) \\cdot ln(1-p))$$\n",
    "\n",
    "Причем $p = σ(ω_1 \\cdot x_1 + ... + ω_{n} \\cdot x_n + ω_0 ⋅ 1)$\n",
    "\n",
    "Вспомним простое правило из математического анализа о вычисленнии [производной сложной функции](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=qny-9SFZUibG):\n",
    "\n",
    "$$\\frac{∂ H}{∂ ω_i} = \\frac{∂ H}{∂ p} \\cdot \\frac{∂ p}{∂ ω_i}$$\n",
    "Посчитаем первое и второе слагаемое по отдельности.\n",
    "- $\\frac{∂ H}{∂ p} = -y ⋅ \\frac{1}{p} + (1-y) \\cdot \\frac{1}{1 - p}$\n",
    "\n",
    "Вспомним одно из основных свойств сигмоидальной функции (его доказательство - полезное упражнение, рекомендуется выполнить его самостоятельно):\n",
    "$σ' = σ \\cdot (1 - σ)$\n",
    "- $\\frac{∂ p}{∂ ω_i} = p⋅(1 - p)⋅x_i$, если $i \\neq 0$\n",
    "- $\\frac{∂ p}{∂ ω_0} = p⋅(1 - p)$\n",
    "\n",
    "Введём обозначение $x_0 ≡ 1$\n",
    "\n",
    "Тогда оба равенства можно записать $\\frac{∂ p}{∂ ω_i} = p⋅(1 - p)⋅x_i$\n",
    "\n",
    "Перемножим полученные результаты:\n",
    "\n",
    "$$\\frac{∂ H}{∂ ω_i} = p⋅(1 - p)⋅x_i ⋅ (-y ⋅ \\frac{1}{p} + (1-y) \\cdot \\frac{1}{1 - p}) = x_i((1 - y)⋅p - y⋅(1-p))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подставляя в эту формулу данные из условия задачи, получаем что ответ -0.25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
